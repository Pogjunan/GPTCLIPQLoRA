Foundational Models & Multimodal Systems Learning 프로젝트

AI 연구원 되기위해 기반을 다지기 위한 개인 프로젝트를 진행하고 있습니다. 
이 프로젝트는 단순한 라이브러리 활용을 넘어딥러닝 모델의 근본적인 구조를 이해하고 직접 구현하며 이를 멀티모달 시스템으로 확장하는 데 목표를 둡니다.

제시된 기간은 잠정적인 목표이며 각 주제의 깊이에 따라 유동적으로 조절될 예정입니다. 이 과정을 통해 대규모 언어 모델(LLM)과 멀티모달 아키텍처의 핵심 원리를 논문을 통해 배우고 실용적인 현 딥러닝 및 Ai의 현기술에 대해 알고 앞으로의 문제 해결 능력을 기르고자 합니다.

주요 개발 계획
1. GPT-2 mini: From-Scratch 구현 (약 1-2주)
가장 먼저, 생성 모델의 근간을 이루는 GPT-2 모델을 논문을 기반으로 직접 구현합니다. PyTorch와 Docker 환경에서 BPE 토크나이저 구축부터, Transformer 블록, 학습 루프, 그리고 추론을 위한 샘플링까지 전 과정을 밑바닥부터(from-scratch) 개발할 것입니다.

핵심 목표:

모델 아키텍처: d_model=256, n_layer=12, n_head=8 규모(약 22.5M 파라미터)의 GPT-2 모델을 직접 설계하고 구현합니다.

학습 안정화: bf16/amp와 FlashAttention을 활용하여 효율적인 학습을 진행하고, AdamW, cosine learning rate schedule, gradient clipping 등 안정적인 학습을 위한 기법들을 적용합니다. 
( 이를 선정한 이유로는 AdamW 가 자주 등장하는 이유도 궁금하며 bf16 과 같이 실제 제가 논문을 쓰기위해 사용하였던 학습 안정화과정으로 근본적인 사용이유와 여러가지 양자화가 각각의 장단점과 기초 배경지식을 쌓고싶었습니다)

성능 검증: TinyStories, OpenWebText 등 다양한 데이터셋을 활용하여 모델의 Perplexity(PPL)를 측정하고, 실제 텍스트 생성 품질을 정성적으로 평가합니다. 
(실제로 논문을 보면 아랫부분에 성능평가 부분이 존재하고 대부분 벤치마크를 이용합니다. 논문 작성자들이 어떤 방식으로 성능평가하는지를 제가 그 과정을 직접해보기위함과 앞으로의 논문을 볼 때 논문을 쉽게 읽기위함입니다.)

재현성 확보: 하이퍼파라미터 튜닝(learning rate, weight decay) 과정을 기록하가(시간이 오래 걸릴것같으면 튜닝은 진행하지않겠습니다 => 연구에서 제가 실험과 그 과정 기록이 미숙하다는 것을 깨달았기에 해보고싶습니다. + weight decay 의 역할 확실히 알기)

FSDP/ZeRO와 같은 분산 처리 기술을 실험하여 스케일업에 대해 배우기 (이 과정이 어떤건지 한번 배워보려고 가져와봤습니다)

2. Mini-CLIP: Vision-Language Alignment 구현 (약 1주) (멀티모달을 알기위한 핵심기술)
# 이미 CLIP 에 대한 논문은 다 읽었으며 CLIP 의 InfoNCE Loss function 은 backward 의 연쇄법칙에 대해 gradient 를 직접 계산해보아서 시간이 적게 걸릴 것으로 예측하였습니다.

LLM 구현 경험을 바탕으로, 멀티모달의 핵심인 '정렬(Alignment)'을 이해하기 위해 경량 CLIP 모델을 구현합니다. ViT-tiny와 소형 BERT를 이미지와 텍스트 인코더로 사용하여, InfoNCE 손실 함수 기반의 대조 학습(Contrastive Learning)을 직접 수행합니다.

핵심 목표:

대조 학습: 이미지-텍스트 쌍에 대한 양방향 InfoNCE 손실을 구현하고, 역전파 과정을 깊이 있게 이해합니다.

Ablation Study: Temperature(τ)와 배치 사이즈가 학습 안정성 및 성능에 미치는 영향을 실험하고 분석합니다.

3. Vision-Language 결합 파이프라인 구축: LLaVA/BLIP-2 스타일 (약 1주)

사전 학습된 Vision Encoder와 LLM을 효과적으로 '결합(Fusion)'하는 파이프라인을 구축합니다. 이는 멀티모달 모델이 시각 정보를 이해하고 언어로 답변하게 하는 핵심 기술입니다. (멀티모달에 대한 예시 모델로 지식으로써 공부와 구현해보기)

핵심 목표:

결합 아키텍처 비교: 단순한 Projection-head(MLP) 방식과 Cross-Attention/Q-Former와 같은 정교한 방식의 장단점을 파라미터, 연산량(TFLOPs), 성능 측면에서 비교 분석합니다. (architecture 변경은 제가 매우 많이해보았으며 Q-Former 에 대해서 적용해보는것도 궁금합니다)

정량 평가: 소규모 VQA(Visual Question Answering) 데이터셋에서 BLEU, CIDEr, 정확도 등의 지표를 통해 각 아키텍처의 성능을 평가합니다.

4. RAG (Retrieval-Augmented Generation) 시스템 MVP 개발 (약 1주) (최근 RAG를 이용한 프로젝트를 통해 RAG 와 LangChain 에 대한 기반을 다졌고 이를 기반으로 실제 내부 컴포넌트까지 원리를 이해하는 것을 목표로합니다)
5. 
모델이 학습 데이터에 없는 최신 정보를 활용하고, 답변의 근거를 제시할 수 있도록 RAG 시스템의 최소 기능 제품(MVP)을 개발합니다. LangChain과 같은 프레임워크를 활용하되, 내부 컴포넌트의 원리를 이해하는 데 집중합니다.

핵심 목표:

파이프라인 구축: Embedding → Vector DB (HNSW) → Reranker → LLM으로 이어지는 RAG 파이프라인을 설계합니다. (LLM 까지 제가 만든 것을 결합하는 방식을 바탕으로 두는 것을 목표로합니다)

##성능 실험: HyDE(Hypothetical Document Embeddings), 메타데이터 필터링 등 RAG 성능 향상 기법들의 효과를 Recall@K, 지연 시간 관점에서 분석하고 Pareto 곡선을 도출합니다.

5. 최신 LLM 아키텍처 및 최적화 기법 탐구 (약 1주)
대규모 모델 시대의 핵심 엔지니어링 기술들을 직접 구현하거나 실험하며 맛봅니다.

탐구 주제:

MoE (Mixture of Experts): 간단한 Transformer의 FFN 레이어를 여러 개의 '전문가(Expert)'로 나누고, 라우팅 로직(top-1 vs top-2)에 따른 성능 변화를 관찰합니다.

KV-Cache 최적화: Paged-Attention과 같은 기법이 어떻게 메모리 사용량을 줄이는지 수식적으로 계산하고 원리를 정리합니다.

QLoRA: 4-bit 양자화와 LoRA를 결합하여 7B~16B급 모델을 제한된 자원으로 파인튜닝하는 과정을 실습합니다.

6. 최종 통합 데모 및 회고 (약 1주)
지금까지 개발한 모든 컴포넌트(GPT-2 mini, Mini-CLIP, LLaVA-style connector, RAG)를 유기적으로 결합한 통합 데모를 구축합니다. 이 과정을 통해 개별 기술들이 하나의 멀티모달 시스템 안에서 어떻게 상호작용하는지 증명하고, 프로젝트 전반에 대한 소논문 형식의 보고서를 작성하여 성과를 정리할 계획입니다.

이 프로젝트는 멀티모달 연구원이라는 목표를 향한 진지한 첫걸음입니다. 꾸준한 학습과 실험을 통해 이론과 현실의 간극을 메우고, 깊이 있는 전문성을 갖추기 위해 나아가겠습니다.
